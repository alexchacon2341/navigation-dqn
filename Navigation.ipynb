{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below.  Please run the next code cell without making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# please do not modify the line below\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [ 1.          0.          0.          0.          0.84408134  0.          0.\n",
      "  1.          0.          0.0748472   0.          1.          0.          0.\n",
      "  0.25755     1.          0.          0.          0.          0.74177343\n",
      "  0.          1.          0.          0.          0.25854847  0.          0.\n",
      "  1.          0.          0.09355672  0.          1.          0.          0.\n",
      "  0.31969345  0.          0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agent while it is training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:   \n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "print(\"Score: {}\".format (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agent while it is training.  However, **_after training the agent_**, you can download the saved model weights to watch the agent on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Average: 0.00\n",
      "Episode 2 Average: 0.00\n",
      "Episode 3 Average: -0.33\n",
      "Episode 4 Average: -0.50\n",
      "Episode 5 Average: -0.60\n",
      "Episode 6 Average: -0.83\n",
      "Episode 7 Average: -0.57\n",
      "Episode 8 Average: -0.50\n",
      "Episode 9 Average: -0.44\n",
      "Episode 10 Average: -0.30\n",
      "Episode 11 Average: -0.45\n",
      "Episode 12 Average: -0.42\n",
      "Episode 13 Average: -0.08\n",
      "Episode 14 Average: -0.07\n",
      "Episode 15 Average: -0.07\n",
      "Episode 16 Average: -0.06\n",
      "Episode 17 Average: 0.00\n",
      "Episode 18 Average: 0.06\n",
      "Episode 19 Average: 0.11\n",
      "Episode 20 Average: 0.30\n",
      "Episode 21 Average: 0.29\n",
      "Episode 22 Average: 0.27\n",
      "Episode 23 Average: 0.26\n",
      "Episode 24 Average: 0.21\n",
      "Episode 25 Average: 0.24\n",
      "Episode 26 Average: 0.27\n",
      "Episode 27 Average: 0.19\n",
      "Episode 28 Average: 0.18\n",
      "Episode 29 Average: 0.24\n",
      "Episode 30 Average: 0.33\n",
      "Episode 31 Average: 0.32\n",
      "Episode 32 Average: 0.31\n",
      "Episode 33 Average: 0.30\n",
      "Episode 34 Average: 0.32\n",
      "Episode 35 Average: 0.31\n",
      "Episode 36 Average: 0.36\n",
      "Episode 37 Average: 0.32\n",
      "Episode 38 Average: 0.32\n",
      "Episode 39 Average: 0.36\n",
      "Episode 40 Average: 0.38\n",
      "Episode 41 Average: 0.37\n",
      "Episode 42 Average: 0.36\n",
      "Episode 43 Average: 0.35\n",
      "Episode 44 Average: 0.34\n",
      "Episode 45 Average: 0.33\n",
      "Episode 46 Average: 0.33\n",
      "Episode 47 Average: 0.36\n",
      "Episode 48 Average: 0.35\n",
      "Episode 49 Average: 0.39\n",
      "Episode 50 Average: 0.38\n",
      "Episode 51 Average: 0.37\n",
      "Episode 52 Average: 0.40\n",
      "Episode 53 Average: 0.45\n",
      "Episode 54 Average: 0.44\n",
      "Episode 55 Average: 0.40\n",
      "Episode 56 Average: 0.41\n",
      "Episode 57 Average: 0.44\n",
      "Episode 58 Average: 0.45\n",
      "Episode 59 Average: 0.47\n",
      "Episode 60 Average: 0.45\n",
      "Episode 61 Average: 0.48\n",
      "Episode 62 Average: 0.47\n",
      "Episode 63 Average: 0.51\n",
      "Episode 64 Average: 0.52\n",
      "Episode 65 Average: 0.52\n",
      "Episode 66 Average: 0.56\n",
      "Episode 67 Average: 0.55\n",
      "Episode 68 Average: 0.62\n",
      "Episode 69 Average: 0.61\n",
      "Episode 70 Average: 0.60\n",
      "Episode 71 Average: 0.59\n",
      "Episode 72 Average: 0.60\n",
      "Episode 73 Average: 0.60\n",
      "Episode 74 Average: 0.59\n",
      "Episode 75 Average: 0.63\n",
      "Episode 76 Average: 0.62\n",
      "Episode 77 Average: 0.65\n",
      "Episode 78 Average: 0.68\n",
      "Episode 79 Average: 0.72\n",
      "Episode 80 Average: 0.75\n",
      "Episode 81 Average: 0.75\n",
      "Episode 82 Average: 0.76\n",
      "Episode 83 Average: 0.82\n",
      "Episode 84 Average: 0.82\n",
      "Episode 85 Average: 0.80\n",
      "Episode 86 Average: 0.84\n",
      "Episode 87 Average: 0.86\n",
      "Episode 88 Average: 0.92\n",
      "Episode 89 Average: 0.92\n",
      "Episode 90 Average: 0.94\n",
      "Episode 91 Average: 0.97\n",
      "Episode 92 Average: 0.99\n",
      "Episode 93 Average: 0.98\n",
      "Episode 94 Average: 0.98\n",
      "Episode 95 Average: 1.00\n",
      "Episode 96 Average: 1.00\n",
      "Episode 97 Average: 1.00\n",
      "Episode 98 Average: 0.98\n",
      "Episode 99 Average: 0.99\n",
      "Episode 100 Average: 1.00\n",
      "\tEpisode 100 Average Score: 1.00\n",
      "Episode 101 Average: 1.04\n",
      "Episode 102 Average: 1.10\n",
      "Episode 103 Average: 1.11\n",
      "Episode 104 Average: 1.17\n",
      "Episode 105 Average: 1.20\n",
      "Episode 106 Average: 1.24\n",
      "Episode 107 Average: 1.22\n",
      "Episode 108 Average: 1.27\n",
      "Episode 109 Average: 1.32\n",
      "Episode 110 Average: 1.31\n",
      "Episode 111 Average: 1.33\n",
      "Episode 112 Average: 1.39\n",
      "Episode 113 Average: 1.37\n",
      "Episode 114 Average: 1.38\n",
      "Episode 115 Average: 1.43\n",
      "Episode 116 Average: 1.47\n",
      "Episode 117 Average: 1.51\n",
      "Episode 118 Average: 1.55\n",
      "Episode 119 Average: 1.54\n",
      "Episode 120 Average: 1.51\n",
      "Episode 121 Average: 1.54\n",
      "Episode 122 Average: 1.62\n",
      "Episode 123 Average: 1.66\n",
      "Episode 124 Average: 1.68\n",
      "Episode 125 Average: 1.73\n",
      "Episode 126 Average: 1.75\n",
      "Episode 127 Average: 1.78\n",
      "Episode 128 Average: 1.82\n",
      "Episode 129 Average: 1.84\n",
      "Episode 130 Average: 1.87\n",
      "Episode 131 Average: 1.87\n",
      "Episode 132 Average: 1.88\n",
      "Episode 133 Average: 1.89\n",
      "Episode 134 Average: 1.91\n",
      "Episode 135 Average: 1.94\n",
      "Episode 136 Average: 1.95\n",
      "Episode 137 Average: 2.00\n",
      "Episode 138 Average: 2.04\n",
      "Episode 139 Average: 2.05\n",
      "Episode 140 Average: 2.08\n",
      "Episode 141 Average: 2.12\n",
      "Episode 142 Average: 2.12\n",
      "Episode 143 Average: 2.16\n",
      "Episode 144 Average: 2.20\n",
      "Episode 145 Average: 2.23\n",
      "Episode 146 Average: 2.30\n",
      "Episode 147 Average: 2.32\n",
      "Episode 148 Average: 2.40\n",
      "Episode 149 Average: 2.42\n",
      "Episode 150 Average: 2.45\n",
      "Episode 151 Average: 2.52\n",
      "Episode 152 Average: 2.56\n",
      "Episode 153 Average: 2.56\n",
      "Episode 154 Average: 2.56\n",
      "Episode 155 Average: 2.64\n",
      "Episode 156 Average: 2.68\n",
      "Episode 157 Average: 2.73\n",
      "Episode 158 Average: 2.78\n",
      "Episode 159 Average: 2.83\n",
      "Episode 160 Average: 2.90\n",
      "Episode 161 Average: 2.93\n",
      "Episode 162 Average: 2.98\n",
      "Episode 163 Average: 2.97\n",
      "Episode 164 Average: 3.02\n",
      "Episode 165 Average: 3.04\n",
      "Episode 166 Average: 3.05\n",
      "Episode 167 Average: 3.09\n",
      "Episode 168 Average: 3.08\n",
      "Episode 169 Average: 3.14\n",
      "Episode 170 Average: 3.21\n",
      "Episode 171 Average: 3.25\n",
      "Episode 172 Average: 3.29\n",
      "Episode 173 Average: 3.33\n",
      "Episode 174 Average: 3.36\n",
      "Episode 175 Average: 3.37\n",
      "Episode 176 Average: 3.46\n",
      "Episode 177 Average: 3.52\n",
      "Episode 178 Average: 3.56\n",
      "Episode 179 Average: 3.59\n",
      "Episode 180 Average: 3.64\n",
      "Episode 181 Average: 3.73\n",
      "Episode 182 Average: 3.78\n",
      "Episode 183 Average: 3.84\n",
      "Episode 184 Average: 3.93\n",
      "Episode 185 Average: 4.00\n",
      "Episode 186 Average: 3.97\n",
      "Episode 187 Average: 3.99\n",
      "Episode 188 Average: 4.02\n",
      "Episode 189 Average: 4.05\n",
      "Episode 190 Average: 4.05\n",
      "Episode 191 Average: 4.08\n",
      "Episode 192 Average: 4.14\n",
      "Episode 193 Average: 4.22\n",
      "Episode 194 Average: 4.27\n",
      "Episode 195 Average: 4.31\n",
      "Episode 196 Average: 4.32\n",
      "Episode 197 Average: 4.35\n",
      "Episode 198 Average: 4.44\n",
      "Episode 199 Average: 4.46\n",
      "Episode 200 Average: 4.52\n",
      "\tEpisode 200 Average Score: 4.52\n",
      "Episode 201 Average: 4.55\n",
      "Episode 202 Average: 4.58\n",
      "Episode 203 Average: 4.63\n",
      "Episode 204 Average: 4.67\n",
      "Episode 205 Average: 4.68\n",
      "Episode 206 Average: 4.74\n",
      "Episode 207 Average: 4.81\n",
      "Episode 208 Average: 4.85\n",
      "Episode 209 Average: 4.89\n",
      "Episode 210 Average: 4.99\n",
      "Episode 211 Average: 5.10\n",
      "Episode 212 Average: 5.16\n",
      "Episode 213 Average: 5.18\n",
      "Episode 214 Average: 5.20\n",
      "Episode 215 Average: 5.25\n",
      "Episode 216 Average: 5.32\n",
      "Episode 217 Average: 5.38\n",
      "Episode 218 Average: 5.39\n",
      "Episode 219 Average: 5.44\n",
      "Episode 220 Average: 5.55\n",
      "Episode 221 Average: 5.62\n",
      "Episode 222 Average: 5.60\n",
      "Episode 223 Average: 5.65\n",
      "Episode 224 Average: 5.72\n",
      "Episode 225 Average: 5.82\n",
      "Episode 226 Average: 5.89\n",
      "Episode 227 Average: 5.94\n",
      "Episode 228 Average: 5.98\n",
      "Episode 229 Average: 6.00\n",
      "Episode 230 Average: 6.05\n",
      "Episode 231 Average: 6.11\n",
      "Episode 232 Average: 6.16\n",
      "Episode 233 Average: 6.23\n",
      "Episode 234 Average: 6.25\n",
      "Episode 235 Average: 6.32\n",
      "Episode 236 Average: 6.38\n",
      "Episode 237 Average: 6.41\n",
      "Episode 238 Average: 6.45\n",
      "Episode 239 Average: 6.49\n",
      "Episode 240 Average: 6.51\n",
      "Episode 241 Average: 6.53\n",
      "Episode 242 Average: 6.56\n",
      "Episode 243 Average: 6.59\n",
      "Episode 244 Average: 6.65\n",
      "Episode 245 Average: 6.72\n",
      "Episode 246 Average: 6.74\n",
      "Episode 247 Average: 6.78\n",
      "Episode 248 Average: 6.77\n",
      "Episode 249 Average: 6.81\n",
      "Episode 250 Average: 6.84\n",
      "Episode 251 Average: 6.78\n",
      "Episode 252 Average: 6.78\n",
      "Episode 253 Average: 6.85\n",
      "Episode 254 Average: 6.96\n",
      "Episode 255 Average: 6.99\n",
      "Episode 256 Average: 7.02\n",
      "Episode 257 Average: 7.01\n",
      "Episode 258 Average: 6.99\n",
      "Episode 259 Average: 7.01\n",
      "Episode 260 Average: 7.13\n",
      "Episode 261 Average: 7.18\n",
      "Episode 262 Average: 7.20\n",
      "Episode 263 Average: 7.29\n",
      "Episode 264 Average: 7.35\n",
      "Episode 265 Average: 7.42\n",
      "Episode 266 Average: 7.44\n",
      "Episode 267 Average: 7.48\n",
      "Episode 268 Average: 7.56\n",
      "Episode 269 Average: 7.55\n",
      "Episode 270 Average: 7.56\n",
      "Episode 271 Average: 7.57\n",
      "Episode 272 Average: 7.62\n",
      "Episode 273 Average: 7.68\n",
      "Episode 274 Average: 7.68\n",
      "Episode 275 Average: 7.70\n",
      "Episode 276 Average: 7.72\n",
      "Episode 277 Average: 7.72\n",
      "Episode 278 Average: 7.76\n",
      "Episode 279 Average: 7.77\n",
      "Episode 280 Average: 7.83\n",
      "Episode 281 Average: 7.79\n",
      "Episode 282 Average: 7.80\n",
      "Episode 283 Average: 7.78\n",
      "Episode 284 Average: 7.71\n",
      "Episode 285 Average: 7.76\n",
      "Episode 286 Average: 7.84\n",
      "Episode 287 Average: 7.85\n",
      "Episode 288 Average: 7.86\n",
      "Episode 289 Average: 7.89\n",
      "Episode 290 Average: 7.99\n",
      "Episode 291 Average: 8.02\n",
      "Episode 292 Average: 8.02\n",
      "Episode 293 Average: 8.04\n",
      "Episode 294 Average: 8.02\n",
      "Episode 295 Average: 8.07\n",
      "Episode 296 Average: 8.15\n",
      "Episode 297 Average: 8.15\n",
      "Episode 298 Average: 8.16\n",
      "Episode 299 Average: 8.21\n",
      "Episode 300 Average: 8.26\n",
      "\tEpisode 300 Average Score: 8.26\n",
      "Episode 301 Average: 8.24\n",
      "Episode 302 Average: 8.21\n",
      "Episode 303 Average: 8.24\n",
      "Episode 304 Average: 8.20\n",
      "Episode 305 Average: 8.23\n",
      "Episode 306 Average: 8.27\n",
      "Episode 307 Average: 8.32\n",
      "Episode 308 Average: 8.40\n",
      "Episode 309 Average: 8.42\n",
      "Episode 310 Average: 8.41\n",
      "Episode 311 Average: 8.40\n",
      "Episode 312 Average: 8.35\n",
      "Episode 313 Average: 8.38\n",
      "Episode 314 Average: 8.46\n",
      "Episode 315 Average: 8.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 316 Average: 8.32\n",
      "Episode 317 Average: 8.33\n",
      "Episode 318 Average: 8.42\n",
      "Episode 319 Average: 8.46\n",
      "Episode 320 Average: 8.36\n",
      "Episode 321 Average: 8.32\n",
      "Episode 322 Average: 8.33\n",
      "Episode 323 Average: 8.28\n",
      "Episode 324 Average: 8.31\n",
      "Episode 325 Average: 8.30\n",
      "Episode 326 Average: 8.25\n",
      "Episode 327 Average: 8.27\n",
      "Episode 328 Average: 8.30\n",
      "Episode 329 Average: 8.34\n",
      "Episode 330 Average: 8.32\n",
      "Episode 331 Average: 8.39\n",
      "Episode 332 Average: 8.40\n",
      "Episode 333 Average: 8.40\n",
      "Episode 334 Average: 8.42\n",
      "Episode 335 Average: 8.37\n",
      "Episode 336 Average: 8.32\n",
      "Episode 337 Average: 8.32\n",
      "Episode 338 Average: 8.33\n",
      "Episode 339 Average: 8.37\n",
      "Episode 340 Average: 8.38\n",
      "Episode 341 Average: 8.42\n",
      "Episode 342 Average: 8.50\n",
      "Episode 343 Average: 8.58\n",
      "Episode 344 Average: 8.61\n",
      "Episode 345 Average: 8.56\n",
      "Episode 346 Average: 8.50\n",
      "Episode 347 Average: 8.56\n",
      "Episode 348 Average: 8.56\n",
      "Episode 349 Average: 8.62\n",
      "Episode 350 Average: 8.64\n",
      "Episode 351 Average: 8.78\n",
      "Episode 352 Average: 8.84\n",
      "Episode 353 Average: 8.83\n",
      "Episode 354 Average: 8.82\n",
      "Episode 355 Average: 8.80\n",
      "Episode 356 Average: 8.86\n",
      "Episode 357 Average: 8.87\n",
      "Episode 358 Average: 8.93\n",
      "Episode 359 Average: 8.94\n",
      "Episode 360 Average: 8.87\n",
      "Episode 361 Average: 8.89\n",
      "Episode 362 Average: 8.91\n",
      "Episode 363 Average: 8.87\n",
      "Episode 364 Average: 8.87\n",
      "Episode 365 Average: 8.83\n",
      "Episode 366 Average: 8.88\n",
      "Episode 367 Average: 8.84\n",
      "Episode 368 Average: 8.86\n",
      "Episode 369 Average: 8.91\n",
      "Episode 370 Average: 8.94\n",
      "Episode 371 Average: 8.98\n",
      "Episode 372 Average: 9.02\n",
      "Episode 373 Average: 9.05\n",
      "Episode 374 Average: 9.13\n",
      "Episode 375 Average: 9.15\n",
      "Episode 376 Average: 9.13\n",
      "Episode 377 Average: 9.16\n",
      "Episode 378 Average: 9.15\n",
      "Episode 379 Average: 9.19\n",
      "Episode 380 Average: 9.17\n",
      "Episode 381 Average: 9.25\n",
      "Episode 382 Average: 9.35\n",
      "Episode 383 Average: 9.37\n",
      "Episode 384 Average: 9.42\n",
      "Episode 385 Average: 9.44\n",
      "Episode 386 Average: 9.49\n",
      "Episode 387 Average: 9.58\n",
      "Episode 388 Average: 9.60\n",
      "Episode 389 Average: 9.63\n",
      "Episode 390 Average: 9.60\n",
      "Episode 391 Average: 9.62\n",
      "Episode 392 Average: 9.64\n",
      "Episode 393 Average: 9.67\n",
      "Episode 394 Average: 9.72\n",
      "Episode 395 Average: 9.69\n",
      "Episode 396 Average: 9.68\n",
      "Episode 397 Average: 9.81\n",
      "Episode 398 Average: 9.81\n",
      "Episode 399 Average: 9.86\n",
      "Episode 400 Average: 9.80\n",
      "\tEpisode 400 Average Score: 9.80\n",
      "Episode 401 Average: 9.92\n",
      "Episode 402 Average: 9.91\n",
      "Episode 403 Average: 9.89\n",
      "Episode 404 Average: 9.97\n",
      "Episode 405 Average: 10.04\n",
      "Episode 406 Average: 10.03\n",
      "Episode 407 Average: 10.03\n",
      "Episode 408 Average: 9.99\n",
      "Episode 409 Average: 9.99\n",
      "Episode 410 Average: 10.05\n",
      "Episode 411 Average: 10.02\n",
      "Episode 412 Average: 10.07\n",
      "Episode 413 Average: 10.11\n",
      "Episode 414 Average: 10.11\n",
      "Episode 415 Average: 10.23\n",
      "Episode 416 Average: 10.29\n",
      "Episode 417 Average: 10.18\n",
      "Episode 418 Average: 10.13\n",
      "Episode 419 Average: 10.16\n",
      "Episode 420 Average: 10.27\n",
      "Episode 421 Average: 10.30\n",
      "Episode 422 Average: 10.42\n",
      "Episode 423 Average: 10.48\n",
      "Episode 424 Average: 10.47\n",
      "Episode 425 Average: 10.40\n",
      "Episode 426 Average: 10.43\n",
      "Episode 427 Average: 10.47\n",
      "Episode 428 Average: 10.57\n",
      "Episode 429 Average: 10.62\n",
      "Episode 430 Average: 10.65\n",
      "Episode 431 Average: 10.65\n",
      "Episode 432 Average: 10.67\n",
      "Episode 433 Average: 10.68\n",
      "Episode 434 Average: 10.66\n",
      "Episode 435 Average: 10.68\n",
      "Episode 436 Average: 10.78\n",
      "Episode 437 Average: 10.80\n",
      "Episode 438 Average: 10.85\n",
      "Episode 439 Average: 10.89\n",
      "Episode 440 Average: 10.99\n",
      "Episode 441 Average: 11.10\n",
      "Episode 442 Average: 11.07\n",
      "Episode 443 Average: 11.03\n",
      "Episode 444 Average: 11.10\n",
      "Episode 445 Average: 11.21\n",
      "Episode 446 Average: 11.35\n",
      "Episode 447 Average: 11.26\n",
      "Episode 448 Average: 11.35\n",
      "Episode 449 Average: 11.34\n",
      "Episode 450 Average: 11.37\n",
      "Episode 451 Average: 11.34\n",
      "Episode 452 Average: 11.31\n",
      "Episode 453 Average: 11.40\n",
      "Episode 454 Average: 11.32\n",
      "Episode 455 Average: 11.39\n",
      "Episode 456 Average: 11.37\n",
      "Episode 457 Average: 11.46\n",
      "Episode 458 Average: 11.45\n",
      "Episode 459 Average: 11.47\n",
      "Episode 460 Average: 11.53\n",
      "Episode 461 Average: 11.54\n",
      "Episode 462 Average: 11.62\n",
      "Episode 463 Average: 11.68\n",
      "Episode 464 Average: 11.64\n",
      "Episode 465 Average: 11.70\n",
      "Episode 466 Average: 11.76\n",
      "Episode 467 Average: 11.78\n",
      "Episode 468 Average: 11.72\n",
      "Episode 469 Average: 11.75\n",
      "Episode 470 Average: 11.79\n",
      "Episode 471 Average: 11.82\n",
      "Episode 472 Average: 11.73\n",
      "Episode 473 Average: 11.76\n",
      "Episode 474 Average: 11.70\n",
      "Episode 475 Average: 11.75\n",
      "Episode 476 Average: 11.71\n",
      "Episode 477 Average: 11.71\n",
      "Episode 478 Average: 11.75\n",
      "Episode 479 Average: 11.79\n",
      "Episode 480 Average: 11.81\n",
      "Episode 481 Average: 11.81\n",
      "Episode 482 Average: 11.74\n",
      "Episode 483 Average: 11.75\n",
      "Episode 484 Average: 11.74\n",
      "Episode 485 Average: 11.73\n",
      "Episode 486 Average: 11.69\n",
      "Episode 487 Average: 11.69\n",
      "Episode 488 Average: 11.72\n",
      "Episode 489 Average: 11.75\n",
      "Episode 490 Average: 11.76\n",
      "Episode 491 Average: 11.72\n",
      "Episode 492 Average: 11.81\n",
      "Episode 493 Average: 11.83\n",
      "Episode 494 Average: 11.86\n",
      "Episode 495 Average: 11.86\n",
      "Episode 496 Average: 11.90\n",
      "Episode 497 Average: 11.86\n",
      "Episode 498 Average: 11.88\n",
      "Episode 499 Average: 11.91\n",
      "Episode 500 Average: 11.98\n",
      "\tEpisode 500 Average Score: 11.98\n",
      "Episode 501 Average: 11.92\n",
      "Episode 502 Average: 12.01\n",
      "Episode 503 Average: 12.10\n",
      "Episode 504 Average: 12.14\n",
      "Episode 505 Average: 12.07\n",
      "Episode 506 Average: 12.08\n",
      "Episode 507 Average: 12.09\n",
      "Episode 508 Average: 12.07\n",
      "Episode 509 Average: 12.11\n",
      "Episode 510 Average: 12.06\n",
      "Episode 511 Average: 12.08\n",
      "Episode 512 Average: 12.10\n",
      "Episode 513 Average: 12.15\n",
      "Episode 514 Average: 12.14\n",
      "Episode 515 Average: 12.14\n",
      "Episode 516 Average: 12.14\n",
      "Episode 517 Average: 12.28\n",
      "Episode 518 Average: 12.30\n",
      "Episode 519 Average: 12.30\n",
      "Episode 520 Average: 12.30\n",
      "Episode 521 Average: 12.36\n",
      "Episode 522 Average: 12.20\n",
      "Episode 523 Average: 12.26\n",
      "Episode 524 Average: 12.20\n",
      "Episode 525 Average: 12.24\n",
      "Episode 526 Average: 12.25\n",
      "Episode 527 Average: 12.29\n",
      "Episode 528 Average: 12.29\n",
      "Episode 529 Average: 12.26\n",
      "Episode 530 Average: 12.26\n",
      "Episode 531 Average: 12.27\n",
      "Episode 532 Average: 12.31\n",
      "Episode 533 Average: 12.40\n",
      "Episode 534 Average: 12.50\n",
      "Episode 535 Average: 12.56\n",
      "Episode 536 Average: 12.53\n",
      "Episode 537 Average: 12.62\n",
      "Episode 538 Average: 12.69\n",
      "Episode 539 Average: 12.67\n",
      "Episode 540 Average: 12.67\n",
      "Episode 541 Average: 12.57\n",
      "Episode 542 Average: 12.58\n",
      "Episode 543 Average: 12.60\n",
      "Episode 544 Average: 12.48\n",
      "Episode 545 Average: 12.49\n",
      "Episode 546 Average: 12.44\n",
      "Episode 547 Average: 12.60\n",
      "Episode 548 Average: 12.57\n",
      "Episode 549 Average: 12.55\n",
      "Episode 550 Average: 12.57\n",
      "Episode 551 Average: 12.54\n",
      "Episode 552 Average: 12.58\n",
      "Episode 553 Average: 12.58\n",
      "Episode 554 Average: 12.71\n",
      "Episode 555 Average: 12.73\n",
      "Episode 556 Average: 12.69\n",
      "Episode 557 Average: 12.60\n",
      "Episode 558 Average: 12.62\n",
      "Episode 559 Average: 12.67\n",
      "Episode 560 Average: 12.64\n",
      "Episode 561 Average: 12.64\n",
      "Episode 562 Average: 12.64\n",
      "Episode 563 Average: 12.61\n",
      "Episode 564 Average: 12.71\n",
      "Episode 565 Average: 12.74\n",
      "Episode 566 Average: 12.70\n",
      "Episode 567 Average: 12.80\n",
      "Episode 568 Average: 12.89\n",
      "Episode 569 Average: 12.88\n",
      "Episode 570 Average: 12.81\n",
      "Episode 571 Average: 12.83\n",
      "Episode 572 Average: 13.00\n",
      "\n",
      "Environment solved in 472 episodes!\tAverage Score: 13.00\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "# AGENT\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def act(self, state, eps=0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "agent = Agent(state_size, action_size, seed=0)              # initialize agent\n",
    "    \n",
    "# DQN\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                                            # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)                       # last 100 scores\n",
    "    eps = eps_start                                        # initialize epsilon\n",
    "    for i_episode in range (1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "        for t in range(max_t):   \n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward                                # update the score\n",
    "            state = next_state                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append (score)                       # save most recent score\n",
    "        scores.append(score)                               # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps)                  # decrease epsilon\n",
    "        print(\"Episode {} Average: {:.2f}\".format (i_episode, np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\tEpisode {} Average Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'nav_weights.pth')\n",
    "            break\n",
    "\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
